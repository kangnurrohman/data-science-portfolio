{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kangnurrohman/sentiment-analysis-projects/blob/main/src/polarity-determination-with-lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "<a href=\"https://www.kaggle.com/code/kangnurrohman/polarity-determination-with-lstm?scriptVersionId=112064672\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
      ],
      "metadata": {
        "id": "eBDTyaRDpsbO"
      },
      "cell_type": "markdown"
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-11-10T00:58:34.90234Z",
          "iopub.execute_input": "2022-11-10T00:58:34.903089Z",
          "iopub.status.idle": "2022-11-10T00:58:34.923108Z",
          "shell.execute_reply.started": "2022-11-10T00:58:34.902996Z",
          "shell.execute_reply": "2022-11-10T00:58:34.921967Z"
        },
        "trusted": true,
        "id": "LRWxeHwNpsba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Library"
      ],
      "metadata": {
        "id": "lmtkWLbipsbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-10T00:58:41.641419Z",
          "iopub.execute_input": "2022-11-10T00:58:41.641902Z",
          "iopub.status.idle": "2022-11-10T00:58:51.565534Z",
          "shell.execute_reply.started": "2022-11-10T00:58:41.641861Z",
          "shell.execute_reply": "2022-11-10T00:58:51.563808Z"
        },
        "trusted": true,
        "id": "9lZDIop_psbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Library"
      ],
      "metadata": {
        "id": "jtrGz8d2psbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import tqdm\n",
        "import nltk\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from keras.models import load_model\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-10T00:58:56.172039Z",
          "iopub.execute_input": "2022-11-10T00:58:56.172487Z",
          "iopub.status.idle": "2022-11-10T00:58:58.434874Z",
          "shell.execute_reply.started": "2022-11-10T00:58:56.172449Z",
          "shell.execute_reply": "2022-11-10T00:58:58.433878Z"
        },
        "trusted": true,
        "id": "ys18vYg4psbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "GfMz9nPYpsbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/kaggle/input/review-of-the-application-pln-mobile/review of the application PLN mobile.xlsx')\n",
        "df.rename(columns = {'content':'review', 'score':'sentiment'}, inplace = True)\n",
        "df.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-10T00:59:01.629991Z",
          "iopub.execute_input": "2022-11-10T00:59:01.63066Z"
        },
        "trusted": true,
        "id": "oiFsHzyNpsbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sort_values(by='at', ascending=False)\n",
        "df = df[['review', 'sentiment']]\n",
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "ahXKnAzdpsbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "-K8380Iepsby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.replace({'sentiment' : {1:'negative', 2:'negative', 3:'neutral', 4:'positive', 5: 'positive' }})\n",
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "-FI8Emjapsbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sentiment.value_counts()"
      ],
      "metadata": {
        "trusted": true,
        "id": "T5G_4NZ1psb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removing neutral\n",
        "df = df[df.sentiment != \"neutral\"]"
      ],
      "metadata": {
        "trusted": true,
        "id": "CpnaGexTpsb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "s6BVguNKpsb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_tweet_special(text):\n",
        "    # remove tab, new line, ans back slice\n",
        "    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
        "    # remove non ASCII (emoticon, chinese word, .etc)\n",
        "    text = text.encode('ascii', 'replace').decode('ascii')\n",
        "    # remove mention, link, hashtag\n",
        "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
        "    # remove incomplete URL\n",
        "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
        "\n",
        "def remove_number(text):\n",
        "    return  re.sub(r\"\\d+\", \"\", text)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
        "\n",
        "def remove_whitespace_LT(text):\n",
        "    return text.strip()\n",
        "\n",
        "def remove_whitespace_multiple(text):\n",
        "    return re.sub('\\s+',' ',text)\n",
        "\n",
        "def remove_singl_char(text):\n",
        "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
        "\n",
        "def word_tokenize_wrapper(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "def stopwords_removal(words):\n",
        "    list_stopwords = nltk.corpus.stopwords.words('indonesian')\n",
        "    #list_stopwords = stopwords.words('indonesian')\n",
        "    #list_stopwords.extend([])\n",
        "    #txt_stopword = pd.read_csv(\"#\", names= [\"stopwords\"], header = None)\n",
        "    #list_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
        "    return [word for word in words if word not in list_stopwords]\n",
        "\n",
        "def stemmed_wrapper(term):\n",
        "    factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "    return stemmer.stem(term)"
      ],
      "metadata": {
        "trusted": true,
        "id": "0ZRlL65_psb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process_corpus(docs):\n",
        "  norm_docs = []\n",
        "  for doc in tqdm.tqdm(docs):\n",
        "    #case folding\n",
        "    doc = doc.lower()\n",
        "    doc = doc.lower()\n",
        "    #tokenization\n",
        "    doc = remove_tweet_special(doc)\n",
        "    doc = remove_number(doc)\n",
        "    doc = remove_punctuation(doc)\n",
        "    doc = remove_whitespace_LT(doc)\n",
        "    doc = remove_whitespace_multiple(doc)\n",
        "    doc = remove_singl_char(doc)\n",
        "    doc = word_tokenize_wrapper(doc)\n",
        "    #filtering\n",
        "    doc = stopwords_removal(doc)\n",
        "    #Stemming for indonesian\n",
        "    #doc = stemmed_wrapper(doc)\n",
        "    norm_docs.append(doc)\n",
        "    \n",
        "  norm_docs = [\" \".join(word) for word in norm_docs]\n",
        "  return norm_docs"
      ],
      "metadata": {
        "trusted": true,
        "id": "lNUukXfKpsb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df.review = pre_process_corpus(df.review)"
      ],
      "metadata": {
        "trusted": true,
        "id": "D8l8Bwy0psb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "iGPYZVsxpsb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling imbalance (Oversampling)"
      ],
      "metadata": {
        "id": "vbZDbCdypsb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "# Separate majority and minority classes in training data for upsampling \n",
        "data_majority = df[df['sentiment'] == 'positive']\n",
        "data_minority = df[df['sentiment'] == 'negative']\n",
        "\n",
        "print(\"majority class before upsample:\",data_majority.shape)\n",
        "print(\"minority class before upsample:\",data_minority.shape)\n",
        "\n",
        "# Upsample minority class\n",
        "data_minority_upsampled = resample(data_minority, \n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples= data_majority.shape[0],    # to match majority class\n",
        "                                 random_state=123) # reproducible results\n",
        " \n",
        "# Combine majority class with upsampled minority class\n",
        "df_balance = pd.concat([data_majority, data_minority_upsampled])\n",
        " \n",
        "# Display new class counts\n",
        "print(\"After upsampling\\n\",df_balance.sentiment.value_counts(),sep = \"\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "zRvnCcv3psb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting Data"
      ],
      "metadata": {
        "id": "mt25w9Yhpsb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df_balance.review, df_balance.sentiment, test_size=0.2, random_state=42)\n",
        "X_train.shape , X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "qwi8TaUepsb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data formatting"
      ],
      "metadata": {
        "id": "ar_fPi4Epsb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "\n",
        "t = keras.preprocessing.text.Tokenizer(oov_token='<UNK>')\n",
        "# fit the tokenizer on the documents\n",
        "t.fit_on_texts(X_train)\n",
        "t.word_index['<PAD>'] = 0"
      ],
      "metadata": {
        "trusted": true,
        "id": "PfnX_p8qpscB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), min([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), t.word_index['<UNK>']"
      ],
      "metadata": {
        "trusted": true,
        "id": "VYCT6oqrpscB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequence"
      ],
      "metadata": {
        "id": "IpLoUvCRpscC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train  = t.texts_to_sequences(X_train)\n",
        "X_test = t.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "CEQyykWKpscC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocabulary size={}\".format(len(t.word_index)))\n",
        "print(\"Number of Documents={}\".format(t.document_count))"
      ],
      "metadata": {
        "trusted": true,
        "id": "3Mt8Z4NlpscD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequence Normalization"
      ],
      "metadata": {
        "id": "hP8XOzz6pscD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 100"
      ],
      "metadata": {
        "trusted": true,
        "id": "3Cc7BemgpscE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pad dataset to a maximum review length in words\n",
        "import tensorflow as tf\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "tx_jVdzypscE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding Labels"
      ],
      "metadata": {
        "id": "g64cHhw8pscE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "num_classes=2 # positive -> 1, negative -> 0"
      ],
      "metadata": {
        "trusted": true,
        "id": "YVHzqcqZpscF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = le.fit_transform(y_train)\n",
        "y_test = le.transform(y_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "mCVEfWL8pscF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(t.word_index)"
      ],
      "metadata": {
        "trusted": true,
        "id": "owEFGbFipscG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Model Architecture"
      ],
      "metadata": {
        "id": "yMPzF6pOpscG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 300 # dimension for dense embeddings for each token\n",
        "LSTM_DIM = 128 # total LSTM units\n",
        "\n",
        "inp = keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "x = keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, trainable=True)(inp)\n",
        "x = keras.layers.CuDNNLSTM(LSTM_DIM, return_sequences=True)(x)\n",
        "#x = (keras.layers.LSTM(LSTM_DIM, return_sequences=True)(x)\n",
        "x = keras.layers.Dense(LSTM_DIM, activation='relu')(x)\n",
        "x = keras.layers.Dropout(rate=0.5)(x)\n",
        "x = keras.layers.Dense(LSTM_DIM, activation='relu')(x)\n",
        "x = keras.layers.Dropout(rate=0.5)(x)\n",
        "\n",
        "outp = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "# initialize the model\n",
        "model = keras.models.Model(inputs=inp, outputs=outp)\n",
        "\n",
        "# make the model parallel\n",
        "#model2 = tf.keras.utils.multi_gpu_model(model, gpus=2)\n",
        "    \n",
        "model.compile(loss='binary_crossentropy', optimizer=tf.optimizers.Adam(), metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "trusted": true,
        "id": "vFQRhez3pscH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "C9FlUZIcpscI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "epochs = 300\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)\n",
        "mc = ModelCheckpoint('./best_model/best_model_lstm.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "# fit model\n",
        "history = model.fit(X_train, y_train,  batch_size=batch_size, shuffle=True, validation_split=0.1, epochs=epochs, verbose=1, callbacks=[es, mc])"
      ],
      "metadata": {
        "trusted": true,
        "id": "FWqhrPGzpscI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Model Performance"
      ],
      "metadata": {
        "id": "RBo3RWfDpscJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model = load_model('./best_model/best_model_lstm.h5')\n",
        "train_acc = saved_model.evaluate(X_train, y_train, verbose=1)\n",
        "test_acc = saved_model.evaluate(X_test, y_test, verbose=1)\n",
        "print('Train: %.2f%%, Test: %.2f%%' % (train_acc[1]*100, test_acc[1]*100))"
      ],
      "metadata": {
        "trusted": true,
        "id": "bSMyjwagpscJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "wnL8ib_4pscK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_probs = model.predict(X_test, verbose=1).ravel()\n",
        "predictions = [1 if prob > 0.5 else 0 for prob in prediction_probs]"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZXtu0GGJpscK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}