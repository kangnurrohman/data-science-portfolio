{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ainurrohmanbwx/english-to-hindi-machine-translation-bigru-att?scriptVersionId=145084310\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Introduction\n\nIn this project, I have developed a machine translation model designed to translate text from English to Hindi. The model utilizes a Bidirectional GRU (Gated Recurrent Unit) with an integrated Attention Mechanism, optimized using the Adam optimizer, and employs categorical crossentropy as the loss function. The primary goal of this model is to generate accurate and efficient translations between the two languages.","metadata":{}},{"cell_type":"markdown","source":"# Import Data","metadata":{}},{"cell_type":"code","source":"# Disable warning\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=FutureWarning)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T09:28:25.029591Z","iopub.execute_input":"2023-10-03T09:28:25.03026Z","iopub.status.idle":"2023-10-03T09:28:25.039337Z","shell.execute_reply.started":"2023-10-03T09:28:25.030216Z","shell.execute_reply":"2023-10-03T09:28:25.038109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/english-to-hindi-parallel-dataset/newdata.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T09:28:25.066658Z","iopub.execute_input":"2023-10-03T09:28:25.06749Z","iopub.status.idle":"2023-10-03T09:28:27.569374Z","shell.execute_reply.started":"2023-10-03T09:28:25.067458Z","shell.execute_reply":"2023-10-03T09:28:27.568257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-03T09:28:27.571532Z","iopub.execute_input":"2023-10-03T09:28:27.572252Z","iopub.status.idle":"2023-10-03T09:28:27.579337Z","shell.execute_reply.started":"2023-10-03T09:28:27.572217Z","shell.execute_reply":"2023-10-03T09:28:27.578319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"df = df.sample(n=20000, random_state = 42)\n# Filter rows with non-null English sentences\ndf = df[~pd.isnull(df['english_sentence'])]\n# Remove duplicate rows\ndf.drop_duplicates(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T09:28:27.581022Z","iopub.execute_input":"2023-10-03T09:28:27.581771Z","iopub.status.idle":"2023-10-03T09:28:27.67814Z","shell.execute_reply.started":"2023-10-03T09:28:27.581733Z","shell.execute_reply":"2023-10-03T09:28:27.676986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert English and Hindi sentences to lowercase\ndf['english_sentence'] = df['english_sentence'].apply(lambda x: x.lower())\ndf['hindi_sentence'] = df['hindi_sentence'].apply(lambda x: x.lower()) ","metadata":{"execution":{"iopub.status.busy":"2023-10-03T09:28:27.681168Z","iopub.execute_input":"2023-10-03T09:28:27.681887Z","iopub.status.idle":"2023-10-03T09:28:27.766368Z","shell.execute_reply.started":"2023-10-03T09:28:27.681849Z","shell.execute_reply":"2023-10-03T09:28:27.765147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n# Remove single quotes (apostrophes) from English and Hindi sentences\ndf['english_sentence'] = df['english_sentence'].apply(lambda x: re.sub(\"'\", '', x))\ndf['hindi_sentence'] = df['hindi_sentence'].apply(lambda x: re.sub(\"'\", '', x))","metadata":{"execution":{"iopub.status.busy":"2023-10-03T09:28:27.768695Z","iopub.execute_input":"2023-10-03T09:28:27.769106Z","iopub.status.idle":"2023-10-03T09:28:27.844341Z","shell.execute_reply.started":"2023-10-03T09:28:27.769044Z","shell.execute_reply":"2023-10-03T09:28:27.843374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\n\nexclude = set(string.punctuation) # Set of all special characters\n# Remove all the special characters\ndf['english_sentence'] = df['english_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\ndf['hindi_sentence'] = df['hindi_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))","metadata":{"execution":{"iopub.status.busy":"2023-10-03T09:28:27.849102Z","iopub.execute_input":"2023-10-03T09:28:27.851453Z","iopub.status.idle":"2023-10-03T09:28:28.55196Z","shell.execute_reply.started":"2023-10-03T09:28:27.851418Z","shell.execute_reply":"2023-10-03T09:28:28.550951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from string import digits\n\nremove_digits = str.maketrans('', '', digits)  # To remove digits from a sentence\n\ndf['english_sentence'] = df['english_sentence'].apply(lambda x: x.translate(remove_digits))  # Remove digits from English text\ndf['hindi_sentence'] = df['hindi_sentence'].apply(lambda x: x.translate(remove_digits))  # Remove digits from Hindi text\n\ndf['hindi_sentence'] = df['hindi_sentence'].apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))  # Remove other unwanted characters from Hindi sentences\n\ndf['english_sentence'] = df['english_sentence'].apply(lambda x: x.strip())  # Remove start and end whitespaces from English text\ndf['hindi_sentence'] = df['hindi_sentence'].apply(lambda x: x.strip())  # Remove start and end whitespaces from Hindi text\n\ndf['english_sentence'] = df['english_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))  # Remove multiple whitespaces from English text\ndf['hindi_sentence'] = df['hindi_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))  # Remove multiple whitespaces from Hindi text\n\ndf['hindi_sentence'] = df['hindi_sentence'].apply(lambda x: 'START_ ' + x + ' _END')  # Add start and end tokens to target sequences","metadata":{"execution":{"iopub.status.busy":"2023-10-03T09:28:28.553259Z","iopub.execute_input":"2023-10-03T09:28:28.553789Z","iopub.status.idle":"2023-10-03T09:28:29.373861Z","shell.execute_reply.started":"2023-10-03T09:28:28.553755Z","shell.execute_reply":"2023-10-03T09:28:29.37259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract unique English words from all English sentences in the DataFrame\nall_eng_words = set()\nfor eng in df['english_sentence']:\n    for word in eng.split():\n        if word not in all_eng_words:\n            all_eng_words.add(word)\n\n# Extract unique Hindi words from all Hindi sentences in the DataFrame\nall_hindi_words = set()\nfor hin in df['hindi_sentence']:\n    for word in hin.split():\n        if word not in all_hindi_words:\n            all_hindi_words.add(word)\n\n# Add a new column indicating the length of each English sentence in terms of words\ndf['length_eng_sentence'] = df['english_sentence'].apply(lambda x: len(x.split(\" \")))\n\n# Add a new column indicating the length of each Hindi sentence in terms of words\ndf['length_hin_sentence'] = df['hindi_sentence'].apply(lambda x: len(x.split(\" \")))","metadata":{"execution":{"iopub.status.busy":"2023-10-03T09:28:29.375591Z","iopub.execute_input":"2023-10-03T09:28:29.376229Z","iopub.status.idle":"2023-10-03T09:28:29.703421Z","shell.execute_reply.started":"2023-10-03T09:28:29.376195Z","shell.execute_reply":"2023-10-03T09:28:29.702343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import shuffle\n\ndf = df[df['length_eng_sentence'] <= 20] # Maximum length of English sentence\ndf = df[df['length_hin_sentence'] <= 20] # Maximum length of Hindi sentence\n\nmax_length_src = max(df['length_hin_sentence']) # Maximum length of Hindi sentence\nmax_length_tar = max(df['length_eng_sentence']) # Maximum length of English sentence\n\ninput_words = sorted(list(all_eng_words)) # Sorted list of unique English words\ntarget_words = sorted(list(all_hindi_words)) # Sorted list of unique Hindi words\nnum_encoder_tokens = len(all_eng_words) # Number of unique English words\nnum_decoder_tokens = len(all_hindi_words) # Number of unique Hindi words\n\nnum_encoder_tokens += 1 # For zero padding\ninput_token_index = dict([(word, i+1) for i, word in enumerate(input_words)]) # Dictionary containing words and their index in the sorted list of English words\ntarget_token_index = dict([(word, i+1) for i, word in enumerate(target_words)]) # Dictionary containing words and their index in the sorted list of Hindi words\nreverse_input_char_index = dict((i, word) for word, i in input_token_index.items()) # Dictionary containing index and corresponding word in the sorted list of English words\nreverse_target_char_index = dict((i, word) for word, i in target_token_index.items()) # Dictionary containing index and corresponding word in the sorted list of Hindi words\ndf = shuffle(df) # Shuffle the DataFrame rows","metadata":{"execution":{"iopub.status.busy":"2023-10-03T09:28:29.705Z","iopub.execute_input":"2023-10-03T09:28:29.70569Z","iopub.status.idle":"2023-10-03T09:28:30.307126Z","shell.execute_reply.started":"2023-10-03T09:28:29.705654Z","shell.execute_reply":"2023-10-03T09:28:30.306017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX, y = df['english_sentence'], df['hindi_sentence'] # English and Hindi sentences\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42) # Split the data into training and testing sets\n\nX_train.to_pickle('/kaggle/working/X_train.pkl') # Save training data frame\nX_test.to_pickle('/kaggle/working/X_test.pkl') # Save testing data frame","metadata":{"execution":{"iopub.status.busy":"2023-10-03T09:28:30.314219Z","iopub.execute_input":"2023-10-03T09:28:30.316315Z","iopub.status.idle":"2023-10-03T09:28:30.556106Z","shell.execute_reply.started":"2023-10-03T09:28:30.316267Z","shell.execute_reply":"2023-10-03T09:28:30.555145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef generate_batch(X=X_train, y=y_train, batch_size=128):\n    ''' Generate a batch of data '''\n    while True:\n        for j in range(0, len(X), batch_size):\n            encoder_input_data = np.zeros((batch_size, max_length_src), dtype='float32')\n            decoder_input_data = np.zeros((batch_size, max_length_tar), dtype='float32')\n            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens), dtype='float32')\n            for i, (input_text, target_text) in enumerate(zip(X[j:j + batch_size], y[j:j + batch_size])):\n                # Set encoder_input_data and decoder_input_data based on input and target sequences\n                input_words = input_text.split()\n                target_words = target_text.split()\n                for t, word in enumerate(input_words):\n                    if t < max_length_src:\n                        encoder_input_data[i, t] = input_token_index[word]\n\n                for t, word in enumerate(target_words):\n                    if t < max_length_tar:\n                        decoder_input_data[i, t] = target_token_index[word]\n                    if t > 0:\n                        # Offset by one timestep for decoder target data\n                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n\n            yield ([encoder_input_data, decoder_input_data], decoder_target_data)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T09:28:30.560843Z","iopub.execute_input":"2023-10-03T09:28:30.563035Z","iopub.status.idle":"2023-10-03T09:28:30.57332Z","shell.execute_reply.started":"2023-10-03T09:28:30.562998Z","shell.execute_reply":"2023-10-03T09:28:30.5726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\nfrom keras.models import Model\nfrom keras.layers import Input, Bidirectional, GRU, Embedding, Dense, Concatenate, Attention\n\nlatent_dim = 300  # Latent dimensionality of the encoding space\nencoder_inputs = Input(shape=(None,))  # Encoder input sequence\nenc_emb = Embedding(num_encoder_tokens, latent_dim, mask_zero=True)(encoder_inputs)  # Encoder embedding layer\nencoder_gru = Bidirectional(GRU(latent_dim, return_sequences=True, return_state=True))  # Bidirectional GRU layer\nencoder_outputs, forward_h, backward_h = encoder_gru(enc_emb)  # Encoder output and states\n\n# Split the concatenated states into two separate states\nencoder_states = [forward_h, backward_h]  # Encoder states\n\ndecoder_inputs = Input(shape=(None,))  # Decoder input sequence\ndec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero=True)  # Decoder embedding layer\ndec_emb = dec_emb_layer(decoder_inputs)  # Decoder embedding layer\n\ndecoder_gru = Bidirectional(GRU(latent_dim, return_sequences=True, return_state=True))  # Bidirectional GRU layer\ndecoder_outputs, _, _ = decoder_gru(dec_emb, initial_state=encoder_states)  # Decoder output and states\n\n# Apply the Attention layer\nattention_layer = Attention()([decoder_outputs, encoder_outputs])\n\n# Concatenate attention output with decoder outputs\ndecoder_concat = Concatenate(axis=-1)([decoder_outputs, attention_layer])\n\n# Add a Dense layer after attention\ndecoder_dense = Dense(num_decoder_tokens, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_concat)\n\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)  # Model object\nmodel.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['acc'])  # Compile the model\nmodel.summary()  # Print model summary","metadata":{"execution":{"iopub.status.busy":"2023-10-03T09:28:30.574728Z","iopub.execute_input":"2023-10-03T09:28:30.575612Z","iopub.status.idle":"2023-10-03T09:28:46.229777Z","shell.execute_reply.started":"2023-10-03T09:28:30.575577Z","shell.execute_reply":"2023-10-03T09:28:46.228895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint\n\ntrain_samples = len(X_train)  # Number of training samples\nval_samples = len(X_test)  # Number of validation samples\nbatch_size = 128  # Batch size\nepochs = 100  # Number of epochs\n\n# Define the EarlyStopping callback\nearly_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, restore_best_weights=True)\n\n# Define the ModelCheckpoint callback with custom verbose text\nmodel_checkpoint = ModelCheckpoint(\n    '/kaggle/working/best_model.h5',\n    monitor='val_loss',\n    mode='min',\n    save_best_only=True,\n    verbose=1,  # Set verbose to 1 to display custom text\n    verbose_text=\"Epoch {epoch}: val_accuracy improved from {old_val_accuracy:.5f} to {new_val_accuracy:.5f}, saving model to ./best_model/best_model.h5\"\n)\n\n# Train the model with both early stopping and model checkpoint\nhistory = model.fit_generator(generator=generate_batch(X_train, y_train, batch_size=batch_size),\n                    steps_per_epoch=train_samples // batch_size,\n                    epochs=epochs,\n                    validation_data=generate_batch(X_test, y_test, batch_size=batch_size),\n                    validation_steps=val_samples // batch_size,\n                    callbacks=[early_stopping, model_checkpoint])\n\n# Save the entire model, including architecture and weights, if needed\nmodel.save('/kaggle/working/final_model.h5')","metadata":{"execution":{"iopub.status.busy":"2023-10-03T09:28:46.230916Z","iopub.execute_input":"2023-10-03T09:28:46.231328Z","iopub.status.idle":"2023-10-03T10:26:06.320267Z","shell.execute_reply.started":"2023-10-03T09:28:46.231294Z","shell.execute_reply":"2023-10-03T10:26:06.319289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot training loss\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# Plot training accuracy\nplt.plot(history.history['acc'], label='Training Accuracy')\nplt.plot(history.history['val_acc'], label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T10:26:06.322096Z","iopub.execute_input":"2023-10-03T10:26:06.323109Z","iopub.status.idle":"2023-10-03T10:26:22.53387Z","shell.execute_reply.started":"2023-10-03T10:26:06.323035Z","shell.execute_reply":"2023-10-03T10:26:22.532888Z"},"trusted":true},"execution_count":null,"outputs":[]}]}